[build-system]
requires = ["setuptools>=61"]
build-backend = "setuptools.build_meta"

[project]
name = "nano-vllm"
version = "0.2.0"
authors = [{ name = "Xingkai Yu" }]
license = {text = "MIT"}
readme = "README.md"
description = "a lightweight vLLM implementation built from scratch"
requires-python = ">=3.10,<3.13"
dependencies = [
    "torch>=2.4.0",
    # Triton and Flash Attention are optional on ROCm (Python 3.12) and macOS - SDPA fallback used instead.
    # Flash Attention is listed as a PyPI dependency (not a URL wheel) for Linux so that uv
    # can resolve metadata via PyPI's JSON API without downloading the full wheel file.
    # This prevents ~240 MB wheel downloads on macOS ARM64 during cross-platform lock resolution.
    # macOS ARM64 is excluded implicitly (flash-attn has no CUDA/macOS support).
    # Windows: install manually from the pre-built wheel:
    #   pip install "flash-attn @ https://github.com/sdbds/flash-attention-for-windows/releases/download/2.8.2/flash_attn-2.8.2+cu128torch2.7.1cxx11abiFALSEfullbackward-cp311-cp311-win_amd64.whl"
    "triton-windows>=3.0.0,<3.4; sys_platform == 'win32' and python_version == '3.11'",
    "triton>=3.0.0; sys_platform == 'linux' and python_version == '3.11'",
    "transformers>=4.51.0",
    "flash-attn>=2.8.3; sys_platform == 'linux' and python_version == '3.11' and platform_machine == 'x86_64'",
    "xxhash",
]

[project.urls]
Homepage="https://github.com/GeeeekExplorer/nano-vllm"

[tool.setuptools.packages.find]
where = ["."]
include = ["nanovllm*"]
